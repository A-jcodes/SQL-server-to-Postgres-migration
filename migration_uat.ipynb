{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e50c5794",
   "metadata": {},
   "source": [
    "Data Migration: SQL Srve to Postgres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33fce33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install psycopg2\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5f044c",
   "metadata": {},
   "source": [
    "### Load credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "545a3a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eaa2107",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sql_host = os.getenv(\"SQLSERVER_HOST\")\n",
    "sql_db = os.getenv(\"SQLSERVER_DB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f191fbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL SERVER HOST: E\\SQLEXPRESS01\n",
      "SQL SERVER DB: TransactionDB_UAT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"SQL SERVER HOST: {sql_host}\")\n",
    "print(f\"SQL SERVER DB: {sql_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc3f1014",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pg_host = os.getenv(\"POSTGRES_HOST\")\n",
    "pg_port = os.getenv(\"POSTGRES_PORT\")\n",
    "pg_db = os.getenv(\"POSTGRES_DB\")\n",
    "pg_user = os.getenv(\"POSTGRES_USER\")\n",
    "pg_password = os.getenv(\"POSTGRES_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7f67ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSGRES HOST: localhost\n",
      "POSGRES PORT: 5433\n",
      "POSGRES DB: Transaction_UAT\n",
      "POSGRES USER: postgres\n",
      "POSGRES PASSWORD: 972340\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"POSGRES HOST: {pg_host}\")\n",
    "print(f\"POSGRES PORT: {pg_port}\")\n",
    "print(f\"POSGRES DB: {pg_db}\")\n",
    "print(f\"POSGRES USER: {pg_user}\")\n",
    "print(f\"POSGRES PASSWORD: {pg_password}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f365ae",
   "metadata": {},
   "source": [
    "### Connect to SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80d35b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to SQL Server...\n",
      "   Server: E\\SQLEXPRESS01\n",
      "   Database: TransactionDB_UAT\n"
     ]
    }
   ],
   "source": [
    "print(\"Connecting to SQL Server...\")\n",
    "print(f\"   Server: {sql_host}\")\n",
    "print(f\"   Database: {sql_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eae29eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] -> Conenction to SQL Server now live! \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sql_conn_string = (\n",
    "        f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "        f\"SERVER={sql_host};\"\n",
    "        f\"DATABASE={sql_db};\"\n",
    "        f\"Trusted_Connection=yes;\"\n",
    "    )\n",
    "\n",
    "    sql_conn = pyodbc.connect(sql_conn_string)\n",
    "    sql_cursor = sql_conn.cursor()\n",
    "    print(\"[SUCCESS] -> Conenction to SQL Server now live! \")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"SQL Server connection failed: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4939ed92",
   "metadata": {},
   "source": [
    "### Connect to Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18bcedde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to PostgreSQL...\n",
      "   Server: localhost\n",
      "   Database: Transaction_UAT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Connecting to PostgreSQL...\")\n",
    "print(f\"   Server: {pg_host}\")\n",
    "print(f\"   Database: {pg_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc5d49c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to PostgreSQL\n",
      "   Version: PostgreSQL 17.4 on x86_64-windows, compiled by msv...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pg_conn = psycopg2.connect(\n",
    "        host=pg_host,\n",
    "        port=pg_port,\n",
    "        database=pg_db,\n",
    "        user=pg_user,\n",
    "        password=pg_password\n",
    "    )\n",
    "\n",
    "\n",
    "    pg_cursor=pg_conn.cursor()\n",
    "    pg_cursor.execute(\"SELECT version();\")\n",
    "\n",
    "    pg_version = pg_cursor.fetchone()[0]\n",
    "\n",
    "    print(\"Connected to PostgreSQL\")\n",
    "    print(f\"   Version: {pg_version[:50]}...\\n\")\n",
    "\n",
    "\n",
    "except psycopg2.OperationalError as e:\n",
    "    print(f\" Postgres connection failed: {e}\")\n",
    "    print(\"\"\" How to troubleshoot:\n",
    "            > 1. Check Postgres is running\n",
    "            > 2. Verify username + password\n",
    "            > 3. Check database exists\n",
    "          \n",
    "          ... \n",
    "\n",
    "\"\"\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n",
    "    raise "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8123ef92",
   "metadata": {},
   "source": [
    "### Migrating tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a6568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#migatrion order is accroding to foregin key dependencies from the least dependent to most dependent\n",
    "#Migration order\n",
    "#Categories (no dependencies )\n",
    "#Suppliers (no dependencies)\n",
    "#Customers (no dependencies)\n",
    "#Products (depends on Categories and Suppliers)\n",
    "\n",
    "tables_to_migrate = [\n",
    "    \"Categories\",\n",
    "    \"Suppliers\",\n",
    "    \"Customers\",\n",
    "    \"Products\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62078b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table to migrate:\n",
      "   1. Categories\n",
      "   2. Suppliers\n",
      "   3. Customers\n",
      "   4. Products\n",
      "\n",
      "Total no of tables to migrate: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Table to migrate:\")\n",
    "for i, table in enumerate(tables_to_migrate, 1):\n",
    "    print(f\"   {i}. {table}\")\n",
    "\n",
    "total_no_tbls = len(tables_to_migrate)\n",
    "print(f\"\\nTotal no of tables to migrate: {total_no_tbls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4937e41",
   "metadata": {},
   "source": [
    "### Pre migration checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      ">>> Check 1: ROW COUNTS\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\">>> Check 1: ROW COUNTS\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07806eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories                 8 rows\n",
      "Suppliers               5000 rows\n",
      "Customers             900000 rows\n",
      "Products              150000 rows\n",
      "------------------------------\n",
      "TOTAL              1,055,008 rows \n",
      "\n",
      " Baseline captured! \n"
     ]
    }
   ],
   "source": [
    "baseline_counts = {}\n",
    "try:\n",
    "    for table in tables_to_migrate:\n",
    "        quoted_table = f\"[{table}]\"\n",
    "        row_count_query = f\"SELECT COUNT(*) as total_rows FROM {quoted_table}\" \n",
    "        sql_cursor.execute(row_count_query)\n",
    "        count = sql_cursor.fetchone()[0]\n",
    "\n",
    "        baseline_counts[table] = count\n",
    "        print(f\"{table:15} {count:>12} rows\")\n",
    "\n",
    "    total_rows = sum(baseline_counts.values())\n",
    "    print(f\"{'-' * 30}\")\n",
    "    print(f\"{'TOTAL':15} {total_rows:>12,} rows \")\n",
    "    print(\"\\n Baseline captured! \")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to get baseline counts: {e}\")\n",
    "    raise\n",
    "\n",
    "# We will use this baseline to validate our migration results at the end of the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00829827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CHECK 2: NULL CHECKS (CustomerName)\n",
      "\n",
      "CHECK 3: INVALID EMAIL FORMATS CHECK\n",
      "\n",
      "CHECK 4: NEGATIVE PRODUCT PRICES CHECK\n",
      "\n",
      "CHECK 5: NEGATIVE STOCK QUANTITIES CHECK\n",
      "\n",
      "CHECK 6: ORPHANED FOREIGN KEYS CHECK\n",
      "\n",
      "CHECK 7: FUTURES DATES CHECK\n",
      "\n",
      "Data quality issues found (will migrate as-is)\n",
      "    > 4,514 customers with NULL names...\n",
      "    > 8,844 emails with invalid email formats...\n",
      "    > 775 prices contain negative prices...\n",
      "    > 1,467 products with negative stock...\n",
      "    > 24,700 products with orphaned foreign keys...\n",
      "    > 9,076 customers with future creations data later than current date...\n"
     ]
    }
   ],
   "source": [
    "# Next, let's do some basic data quality checks to identify any potential issues before we migrate (we will migrate as-is regardless of the results but at least we will be aware of any potential issues upfront)\n",
    "\n",
    "quality_issues = []\n",
    "\n",
    "try:\n",
    "    print(\"\\nCHECK 2: NULL CHECKS (CustomerName)\")\n",
    "    sql_cursor.execute(\"\"\"SELECT COUNT(*) AS null_count \n",
    "                          FROM Customers \n",
    "                          WHERE CustomerName IS NULL\"\"\")\n",
    "    null_names = sql_cursor.fetchone()[0]\n",
    "    if null_names > 0:\n",
    "        quality_issues.append(f\"    > {null_names:,} customers with NULL names...\")\n",
    "    # print(quality_issues)\n",
    "\n",
    "    \n",
    "    print(\"\\nCHECK 3: INVALID EMAIL FORMATS CHECK\")\n",
    "    sql_cursor.execute(\"\"\"SELECT COUNT(*) AS invalid_email_count \n",
    "                            FROM Customers \n",
    "                            WHERE Email LIKE '%@invalid'  \"\"\")\n",
    "    invalid_emails = sql_cursor.fetchone()[0]\n",
    "    if invalid_emails > 0:\n",
    "        quality_issues.append(f\"    > {invalid_emails:,} emails with invalid email formats...\")\n",
    "    # print(quality_issues)\n",
    "    \n",
    "    print(\"\\nCHECK 4: NEGATIVE PRODUCT PRICES CHECK\")\n",
    "    sql_cursor.execute(\"\"\" SELECT COUNT(*) AS negative_product_prices_count \n",
    "                            FROM Products \n",
    "                            WHERE UnitPrice < 0\n",
    "                       \"\"\")\n",
    "    negative_price = sql_cursor.fetchone()[0]\n",
    "    if negative_price > 0:\n",
    "        quality_issues.append(f\"    > {negative_price:,} prices contain negative prices...\")\n",
    "    # print(quality_issues)\n",
    "\n",
    "\n",
    "    print(\"\\nCHECK 5: NEGATIVE STOCK QUANTITIES CHECK\")\n",
    "    sql_cursor.execute(\"\"\" SELECT COUNT(*) AS negative_stock_quanities_count \n",
    "                            FROM Products \n",
    "                            WHERE StockQuantity < 0\n",
    "                       \"\"\")\n",
    "    negative_stock_quantities = sql_cursor.fetchone()[0]\n",
    "    if negative_stock_quantities > 0:\n",
    "        quality_issues.append(f\"    > {negative_stock_quantities:,} products with negative stock...\")\n",
    "    # print(quality_issues)\n",
    "\n",
    "    print(\"\\nCHECK 6: ORPHANED FOREIGN KEYS CHECK\")\n",
    "    sql_cursor.execute(\"\"\" SELECT COUNT(*) AS orphaned_records \n",
    "                        FROM Products prod\n",
    "                        WHERE NOT EXISTS (SELECT 1\n",
    "                                            FROM Suppliers sup\n",
    "                                            WHERE sup.SupplierID=prod.SupplierID)\n",
    "\"\"\")\n",
    "    orphaned_fks = sql_cursor.fetchone()[0]\n",
    "    if orphaned_fks > 0:\n",
    "        quality_issues.append(f\"    > {orphaned_fks:,} products with orphaned foreign keys...\")\n",
    "    # print(quality_issues) \n",
    "\n",
    "\n",
    "    print(\"\\nCHECK 7: FUTURES DATES CHECK\")\n",
    "    sql_cursor.execute(\"\"\" SELECT COUNT(*) as future_dates_count \n",
    "                            FROM Customers\n",
    "                            WHERE CreatedDate > GETDATE()\n",
    "\"\"\")\n",
    "    future_dates = sql_cursor.fetchone()[0]\n",
    "    if future_dates > 0:\n",
    "        quality_issues.append(f\"    > {future_dates:,} customers with future creations data later than current date...\")\n",
    "    # print(quality_issues)\n",
    "\n",
    "    if quality_issues:\n",
    "        print(\"\\nData quality issues found (will migrate as-is)\")\n",
    "        for issue in quality_issues:\n",
    "            print(issue)\n",
    "    else:\n",
    "        print(\"No data qualituy issues identified!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] ===> Unexpected issue: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b194fd5d",
   "metadata": {},
   "source": [
    "### Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5ee4bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "ANALYZE TABLE SCHEMA\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "## Get the table schemas to help with the migration process (e.g. to identify data types, nullable columns, etc...)\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"ANALYZE TABLE SCHEMA\")\n",
    "print(\"=\" * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f461ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_22472\\3909585054.py:20: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(schema_query, sql_conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Categories\n",
      "----------\n",
      "    COLUMN_NAME DATA_TYPE  CHARACTER_MAXIMUM_LENGTH IS_NULLABLE\n",
      "0    CategoryID       int                       NaN          NO\n",
      "1  CategoryName  nvarchar                      50.0         YES\n",
      "2   Description  nvarchar                      -1.0         YES\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_22472\\3909585054.py:20: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(schema_query, sql_conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Suppliers\n",
      "----------\n",
      "    COLUMN_NAME DATA_TYPE  CHARACTER_MAXIMUM_LENGTH IS_NULLABLE\n",
      "0    SupplierID       int                       NaN          NO\n",
      "1  SupplierName  nvarchar                     150.0         YES\n",
      "2   ContactName  nvarchar                     100.0         YES\n",
      "3       Country  nvarchar                     100.0         YES\n",
      "4         Phone  nvarchar                      20.0         YES\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_22472\\3909585054.py:20: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(schema_query, sql_conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Customers\n",
      "----------\n",
      "    COLUMN_NAME DATA_TYPE  CHARACTER_MAXIMUM_LENGTH IS_NULLABLE\n",
      "0    CustomerID       int                       NaN          NO\n",
      "1  CustomerName  nvarchar                     100.0         YES\n",
      "2         Email  nvarchar                     100.0         YES\n",
      "3         Phone  nvarchar                      20.0         YES\n",
      "4       Country  nvarchar                     100.0         YES\n",
      "5   CreatedDate  datetime                       NaN         YES\n",
      "6      IsActive       bit                       NaN         YES\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_22472\\3909585054.py:20: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(schema_query, sql_conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Products\n",
      "----------\n",
      "     COLUMN_NAME DATA_TYPE  CHARACTER_MAXIMUM_LENGTH IS_NULLABLE\n",
      "0      ProductID       int                       NaN          NO\n",
      "1    ProductName  nvarchar                     200.0         YES\n",
      "2     CategoryID       int                       NaN         YES\n",
      "3     SupplierID       int                       NaN         YES\n",
      "4      UnitPrice     money                       NaN         YES\n",
      "5  StockQuantity       int                       NaN         YES\n",
      "6    CreatedDate  datetime                       NaN         YES\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_schema = {}\n",
    "\n",
    "\n",
    "try:\n",
    "    for table in tables_to_migrate:\n",
    "        schema_query = f\"\"\"\n",
    "            SELECT \n",
    "                COLUMN_NAME, \n",
    "                DATA_TYPE, \n",
    "                CHARACTER_MAXIMUM_LENGTH,\n",
    "                IS_NULLABLE\n",
    "            FROM \n",
    "                INFORMATION_SCHEMA.COLUMNS\n",
    "            WHERE \n",
    "                table_name = '{table}'\n",
    "            ORDER BY \n",
    "                ORDINAL_POSITION\n",
    "\n",
    "\"\"\"\n",
    "        schema_df = pd.read_sql(schema_query, sql_conn)\n",
    "        print(f\"\\n{table}\")\n",
    "        print(\"-\" * 10)\n",
    "        print(schema_df)\n",
    "        table_schema[table] = schema_df\n",
    "        print(\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f17c35",
   "metadata": {},
   "source": [
    "###  Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c5a25d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "DATA TYPE MAPPING\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 65)\n",
    "print(\"DATA TYPE MAPPING\")\n",
    "print(\"=\" * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c665261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data type mappings from SQL Server to Postgres\n",
    "\n",
    "type_mapping = {\n",
    "    'int': 'INTEGER',\n",
    "    'bigint': 'BIGINT',\n",
    "    'smallint': 'SMALLINT',\n",
    "    'tinyint': 'SMALLINT',\n",
    "    'bit': 'BOOLEAN',\n",
    "    'decimal': 'NUMERIC',\n",
    "    'numeric': 'NUMERIC',\n",
    "    'money': 'NUMERIC(19,4)',\n",
    "    'smallmoney': 'NUMERIC(10,4)',\n",
    "    'float': 'DOUBLE PRECISION',\n",
    "    'real': 'REAL',\n",
    "    'datetime': 'TIMESTAMP',\n",
    "    'datetime2': 'TIMESTAMP',\n",
    "    'smalldatetime': 'TIMESTAMP',\n",
    "    'date': 'DATE',\n",
    "    'time': 'TIME',\n",
    "    'char': 'CHAR',\n",
    "    'varchar': 'VARCHAR',\n",
    "    'nchar': 'CHAR',\n",
    "    'nvarchar': 'VARCHAR',\n",
    "    'text': 'TEXT',\n",
    "    'ntext': 'TEXT'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5e70d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Server to PostgreSQL type mapping \n",
      "\n",
      "    int               --->      INTEGER\n",
      "    bigint            --->      BIGINT\n",
      "    smallint          --->      SMALLINT\n",
      "    tinyint           --->      SMALLINT\n",
      "    bit               --->      BOOLEAN\n",
      "    decimal           --->      NUMERIC\n",
      "    numeric           --->      NUMERIC\n",
      "    money             --->      NUMERIC(19,4)\n",
      "    smallmoney        --->      NUMERIC(10,4)\n",
      "    float             --->      DOUBLE PRECISION\n",
      "    real              --->      REAL\n",
      "    datetime          --->      TIMESTAMP\n",
      "    datetime2         --->      TIMESTAMP\n",
      "    smalldatetime     --->      TIMESTAMP\n",
      "    date              --->      DATE\n",
      "    time              --->      TIME\n",
      "    char              --->      CHAR\n",
      "    varchar           --->      VARCHAR\n",
      "    nchar             --->      CHAR\n",
      "    nvarchar          --->      VARCHAR\n",
      "    text              --->      TEXT\n",
      "    ntext             --->      TEXT\n"
     ]
    }
   ],
   "source": [
    "print(\"SQL Server to PostgreSQL type mapping \")\n",
    "print()\n",
    "\n",
    "for sql_type, pg_type in list(type_mapping.items()):\n",
    "    print(f\"    {sql_type:13} --->      {pg_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2c14bc",
   "metadata": {},
   "source": [
    "### Create table schemas in Postgres based on the SQL Server schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "869fac39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " + =======================================================\n",
      "[SUCCESS] ---> All tables created successfully!\n"
     ]
    }
   ],
   "source": [
    "## I will create the tables with the same names and structures as in SQL Server to make it easier for validation later on - I can always refactor the Postgres schemas later if needed but for now I'll keep it simple and consistent with the source\n",
    "\n",
    "try:\n",
    "    for table in tables_to_migrate:\n",
    "\n",
    "        schema = table_schema[table]\n",
    "\n",
    "        pg_table = table.lower()\n",
    "\n",
    "        pg_cursor.execute(f\"DROP TABLE IF EXISTS {pg_table} CASCADE\")\n",
    "\n",
    "        column_definitions = []\n",
    "\n",
    "        for idx, row in schema.iterrows():\n",
    "            col_name = row['COLUMN_NAME'].lower()\n",
    "            sql_type = row['DATA_TYPE']\n",
    "\n",
    "            base_type = sql_type.lower()\n",
    "            pg_type = type_mapping.get(base_type, 'TEXT')      \n",
    "\n",
    "            condition_1 = idx == 0                      # Must be first column in the table\n",
    "            condition_2 = col_name.endswith('id')       # Must end with ID\n",
    "            condition_3 = 'int' in sql_type.lower()     # Must be INT data type\n",
    "\n",
    "            if condition_1 and condition_2 and condition_3:\n",
    "                column_definitions.append(f\"{col_name} SERIAL PRIMARY KEY\")\n",
    "            else:\n",
    "                column_definitions.append(f\"{col_name} {pg_type}\")\n",
    "\n",
    "    \n",
    "        column_string = \",\\n        \".join(column_definitions)\n",
    "        create_query =  f\"\"\" \n",
    "        CREATE TABLE {pg_table} (\n",
    "            {column_string}\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        pg_cursor.execute(create_query)\n",
    "        pg_conn.commit()\n",
    "    \n",
    "    print(\"\\n + \" + \"=\" * 55)\n",
    "    print(\"[SUCCESS] ---> All tables created successfully!\")\n",
    "\n",
    "\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"Postgres experienced an error while creating a table: {e}\")\n",
    "    pg_conn.rollback()\n",
    "    raise \n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected issue: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ae623e",
   "metadata": {},
   "source": [
    "### Load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6863c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test migration with one table to validate the process before doing a full migration of all tables\n",
    "\n",
    "test_table = 'Customers'\n",
    "pg_table = test_table.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be32bb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Read from SQL Server... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_22472\\1989631993.py:4: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  test_df = pd.read_sql(extract_query, sql_conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Read 900000 rows\n",
      "2. Transforming data types...\n",
      "[SUCCESS] ---> Converted IsActive: BIT ---> BOOLEAN\n",
      "3. Prepare the data for loading\n",
      "        Prepared 900,000 rows\n",
      "4. Insert data into PostgreSQL...\n",
      "Loaded 900,000 rows\n",
      "5. Verifying...\n",
      "[SUCCESS] --> Verification passed: 900,000 == 900,000 \n",
      "\n",
      " Customers migration test successfully completed!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"1. Read from SQL Server... \")\n",
    "    extract_query = f\"SELECT * FROM {pg_table}\"\n",
    "    test_df = pd.read_sql(extract_query, sql_conn)\n",
    "\n",
    "    print(f\"        Read {len(test_df)} rows\")\n",
    "\n",
    "\n",
    "    print(\"2. Transforming data types...\")\n",
    "\n",
    "    if 'IsActive' in test_df.columns:\n",
    "        test_df['IsActive'] = test_df['IsActive'].astype('bool')\n",
    "        print(\"[SUCCESS] ---> Converted IsActive: BIT ---> BOOLEAN\")\n",
    "\n",
    "\n",
    "    print(\"3. Prepare the data for loading\")\n",
    "    data_tuples = [tuple(row) for row in test_df.to_numpy()]\n",
    "\n",
    "    columns = [col.lower() for col in test_df.columns]\n",
    "\n",
    "    columns_string = ', '.join(columns)\n",
    "\n",
    "    placeholders = ', '.join(['%s'] * len(columns))\n",
    "\n",
    "    insert_query = f\"\"\"\n",
    "        INSERT INTO {pg_table} ({columns_string})\n",
    "        VALUES %s\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"        Prepared {len(data_tuples):,} rows\")\n",
    "\n",
    "\n",
    "    print(\"4. Insert data into PostgreSQL...\")\n",
    "    execute_values(pg_cursor, insert_query, data_tuples, page_size=1000)\n",
    "    pg_conn.commit()\n",
    "\n",
    "    print(f\"Loaded {len(data_tuples):,} rows\")\n",
    "\n",
    "\n",
    "    print(\"5. Verifying...\")\n",
    "    pg_cursor.execute(f\"SELECT COUNT(*) AS total_rows FROM {pg_table}\")\n",
    "    pg_count = pg_cursor.fetchone()[0]\n",
    "\n",
    "    sql_count = baseline_counts[test_table]\n",
    "\n",
    "    if pg_count == sql_count:\n",
    "        print(f\"[SUCCESS] --> Verification passed: {pg_count:,} == {sql_count:,} \")\n",
    "    else:\n",
    "        print(f\"[FAILED] --> Count mismatch: {pg_count:,} != {sql_count:,}\")\n",
    "\n",
    "    \n",
    "    print(f\"\\n {test_table} migration test successfully completed!\")\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    pg_conn.rollback()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17997e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Migrating Categories --> categories...\n",
      "1. Reading from SQL Server...\n",
      "     Read 8 rows\n",
      "\n",
      "\n",
      "2. Preparing data...\n",
      "        Prepared 8 rows\n",
      "\n",
      "\n",
      "3. Processing bulk load...\n",
      "Failed to migrate 'Categories: relation \"uat.categories\" does not exist\n",
      "' \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_22472\\3710759733.py:13: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  sql_df = pd.read_sql(extract_query, sql_conn)\n"
     ]
    },
    {
     "ename": "UndefinedTable",
     "evalue": "relation \"uat.categories\" does not exist\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUndefinedTable\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3. Processing bulk load...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m pg_cursor\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCREATE SCHEMA IF NOT EXISTS uat;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m \u001b[43mpg_cursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTRUNCATE TABLE uat.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpg_table\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m RESTART IDENTITY CASCADE;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m pg_conn\u001b[38;5;241m.\u001b[39mcommit()\n\u001b[0;32m     31\u001b[0m execute_values(pg_cursor, insert_query, data_tuples, page_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "\u001b[1;31mUndefinedTable\u001b[0m: relation \"uat.categories\" does not exist\n"
     ]
    }
   ],
   "source": [
    "#migating the rest of the tables will follow the same process as above but with additional handling for foreign key relationships and dependencies\n",
    "\n",
    "remaining_tables = [t for t in tables_to_migrate if t != 'Customers']\n",
    "\n",
    "for table in remaining_tables:\n",
    "    pg_table = table.lower()\n",
    "\n",
    "    print(f\"Migrating {table} --> {pg_table}...\")\n",
    "\n",
    "    try:\n",
    "        print(\"1. Reading from SQL Server...\")\n",
    "        extract_query = f\"SELECT * FROM {table}\"\n",
    "        sql_df = pd.read_sql(extract_query, sql_conn)\n",
    "        print(f\"     Read {len(sql_df):,} rows\\n\\n\")\n",
    "\n",
    "        print(\"2. Preparing data...\")\n",
    "        data_tuples = [tuple(row) for row in sql_df.to_numpy()]\n",
    "        columns = [col.lower() for col in sql_df.columns]\n",
    "        columns_string = ', '.join(columns)\n",
    "        insert_query = f\"\"\"\n",
    "                INSERT INTO {pg_table} ({columns_string})\n",
    "                VALUES %s\n",
    "\"\"\"\n",
    "        print(f\"        Prepared {len(data_tuples):,} rows\\n\\n\")\n",
    "\n",
    "\n",
    "        print(\"3. Processing bulk load...\")\n",
    "        pg_cursor.execute(\"CREATE SCHEMA IF NOT EXISTS uat;\")\n",
    "        pg_cursor.execute(f\"TRUNCATE TABLE uat.{pg_table} RESTART IDENTITY CASCADE;\")\n",
    "        pg_conn.commit()\n",
    "        execute_values(pg_cursor, insert_query, data_tuples, page_size=1000)\n",
    "        pg_conn.commit()\n",
    "        print(f\"[SUCCESS] --> Loaded {len(data_tuples):,} rows\\n\")\n",
    "       \n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "        print(\"5. Verifying...\")\n",
    "        pg_cursor.execute(f\"SELECT COUNT(*) AS total_rows FROM {pg_table}\")\n",
    "        pg_count = pg_cursor.fetchone()[0]\n",
    "\n",
    "        sql_count = baseline_counts[table]\n",
    "\n",
    "        if pg_count == sql_count:\n",
    "            print(f\"[SUCCESS] --> Verification passed: {pg_count:,} == {sql_count:,} \")\n",
    "        else:\n",
    "            print(f\"[FAILED] --> Count mismatch: {pg_count:,} != {sql_count:,}\")\n",
    "\n",
    "        \n",
    "        print(f\"\\n {table} migration successfully completed!\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to migrate '{table}: {e}' \")\n",
    "        pg_conn.rollback()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd63833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
